{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"NNoM: The Neural Network on Microcontroller. NNoM is a higher-level layer-based Neural Network library specifically for microcontrollers. Highlights Deploy Keras model to NNoM model with one line of code. Support complex structures; Inception, ResNet, DenseNet... High-performance backend selections. Onboard (MCU) evaluation tools; Runtime analysis, Top-k, Confusion matrix... The temporary guide Porting and optimising Guide Why NNoM? The aims of NNoM is to provide a light-weight, user-friendly and flexible interface for fast deploying. Nowadays, neural networks are wider , deeper , and denser . [1] Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., ... & Rabinovich, A. (2015). Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 1-9). [2] He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778). [3] Huang, G., Liu, Z., Van Der Maaten, L., & Weinberger, K. Q. (2017). Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 4700-4708). If you would like to try those more up-to-date, decent and complex structures on MCU NNoM can help you to build them with only a few lines of C codes , same as you did with Python in Keras Dependencies NNoM now use the local pure C backend implementation by default. Thus, there is no special dependency needed. Optimization You can select CMSIS-NN/DSP as the backend for about 5x performance with ARM-Cortex-M4/7/33/35P. Check Porting and optimising Guide for detail.","title":"Overview"},{"location":"#nnom-the-neural-network-on-microcontroller","text":"NNoM is a higher-level layer-based Neural Network library specifically for microcontrollers. Highlights Deploy Keras model to NNoM model with one line of code. Support complex structures; Inception, ResNet, DenseNet... High-performance backend selections. Onboard (MCU) evaluation tools; Runtime analysis, Top-k, Confusion matrix... The temporary guide Porting and optimising Guide","title":"NNoM: The Neural Network on Microcontroller."},{"location":"#why-nnom","text":"The aims of NNoM is to provide a light-weight, user-friendly and flexible interface for fast deploying. Nowadays, neural networks are wider , deeper , and denser . [1] Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., ... & Rabinovich, A. (2015). Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 1-9). [2] He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778). [3] Huang, G., Liu, Z., Van Der Maaten, L., & Weinberger, K. Q. (2017). Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 4700-4708). If you would like to try those more up-to-date, decent and complex structures on MCU NNoM can help you to build them with only a few lines of C codes , same as you did with Python in Keras","title":"Why NNoM?"},{"location":"#dependencies","text":"NNoM now use the local pure C backend implementation by default. Thus, there is no special dependency needed.","title":"Dependencies"},{"location":"#optimization","text":"You can select CMSIS-NN/DSP as the backend for about 5x performance with ARM-Cortex-M4/7/33/35P. Check Porting and optimising Guide for detail.","title":"Optimization"},{"location":"A Temporary Guide to NNoM/","text":"The simplest first. Deploying Deploying is much easier than before. (Thanks to @parai) Simply use generate_model(model, x_data) to generate a C header weights.h after you have trained your model in Keras. It is available in nnom_utils.py Include the weights.h in your project, then call nnom_model_create() to create and compile the model on the MCU. Finaly, call model_run() to do your prediction. Please check MNIST-DenseNet example for usage The generate_model(model, x_data) might not be updated with NNoM from time to time. For new features and customized layers, you can still use NNoM APIs to build your model. NNoM Structure NNoM uses a layer-based structure. A layer is a container. Every operation (convolution, concat...) must be wrapped into a layer. A basic layer contains a list of Input/Ouput modules (I/O). Each of I/O contains a list of Hook (similar to Nodes in Keras). Hook stores the links to an I/O (other layer's) I/O is a buffer to store input/output data of the operation. Dont be scared, check this: Those APIs listed below will help you to create layers and build the model structures. APIs Layer APIs and Construction APIs are used to build a model. Layer APIs can create and return a new layer instance, while construction APIs use layer instances to build a model. Layer APIs such as Conv2D(), Dense(), Activation() ... which you can find in nnom_layers.h Construction APIs such as model.hook(), model.merge(), model.add() ... which you can find in new_model() at nnom.c For example, to add a convolution layer into a sequencial model, use model.add() : model.add(&model, Conv2D(16, kernel(1, 9), stride(1, 2), PADDING_SAME, &c1_w, &c1_b)); In functional model, the links between layer are specified explicitly by using model.hook() or model.merge() x = model.hook(Conv2D(16, kernel(1, 9), stride(1, 2), PADDING_SAME, &c1_w, &c1_b), input_layer); x = model.hook(MaxPool(kernel(1, 2), stride(1, 2), PADDING_VALID), x); NNoM currently supports HWC format. Which also called \"channel last\", where H = number of rows or y axis, W = number of column or x axis, C = number of channes. For example: In the above codes, both kernal(H, W) and stride(H, W) returns a 'shape' instance. The shape instance in format of (H, W, ?) All convolutional layers and poolings layers support both 1D / 2D data input. However, when using 1D input, the H must be set to 1. Construction APIs Construction APIs are statics functions located in nnom.c Currently are: Sequencial Construction API nnom_status_t model.add(nnom_model_t* model, nnom_layer_t *layer); Functional Construction API // hook the current layer to the input layer // this function only to connect (single output layer) to (single input layer). // return the curr (layer) instance nnom_layer_t * model.hook(nnom_layer_t* curr, nnom_layer_t *last) // merge 2 layer's output to one output by provided merging method(a mutiple input layer) // method = merging layer such as (concat(), dot(), mult(), add()) // return the method (layer) instance nnom_layer_t * model.merge(nnom_layer_t *method, nnom_layer_t *in1, nnom_layer_t *in2) // Same as model.merge() // Except it can take mutiple layers as input. // num = the number of layer // method: same as model.merge() nnom_layer_t * model.mergex(nnom_layer_t *method, int num, ...) // This api will merge the activation to the targeted layerto reduce an extra activation layer // activation such as (act_relu(), act_tanh()...) nnom_layer_t * model.active(nnom_activation_t* act, nnom_layer_t * target) For model.active() , please check Activation APIs below. Layer APIs Layers APIs are listed in nnom_layers.h Input/output layers are neccessary for a model. They are responsible to copy data from user's input buffer, and copy out to user's output buffer. // Layer APIs // input/output nnom_layer_t* Input(nnom_shape_t input_shape\uff0cvoid* p_buf); nnom_layer_t* Output(nnom_shape_t output_shape, void* p_buf); Pooling as they are: The sum pooling here will dynamicly change its ourput shift to avoid overflowing. It is recommened to replace the Global Average Pooling by Global Sum Pooling for better accuracy in MCU side. // Pooling, kernel, strides, padding nnom_layer_t* MaxPool(nnom_shape_t k, nnom_shape_t s, nnom_padding_t pad); nnom_layer_t* AvgPool(nnom_shape_t k, nnom_shape_t s, nnom_padding_t pad); nnom_layer_t* SumPool(nnom_shape_t k, nnom_shape_t s, nnom_padding_t pad); // The Global poolings simplly do better nnom_layer_t *GlobalMaxPool(void); nnom_layer_t *GlobalAvgPool(void); nnom_layer_t *GlobalSumPool(void); // Upsampling layer / Unpooling layer nnom_layer_t* UpSample(nnom_shape_t kernel); Activation's Layers API are started with capital letter. They are differed from the Activation API , which start with act_* and retrun an activation instance. Pleas check the Activation APIs below for more detail. They return a layer instance. // Activation layers take activation instance as input. nnom_layer_t* Activation(nnom_activation_t *act); // Activation's layer API. nnom_layer_t* ReLU(void); nnom_layer_t* Softmax(void); nnom_layer_t* Sigmoid(void); nnom_layer_t* TanH(void); Matrix API. These layers normally take 2 or more layer's output as their inputs. They also called \"merging method\", which must be used by model.merge(method, in1, in2) or model.mergex(method, num of input, in1, in2, 1n3 ...) // Matrix nnom_layer_t* Add(void); nnom_layer_t* Sub(void); nnom_layer_t* Mult(void); nnom_layer_t* Concat(int8_t axis); Flatten change the shapes to (x, 1, 1) // utils nnom_layer_t* Flatten(void); Stable NN layers. For more developing layers, please check the source codes. // conv1D/2d nnom_layer_t* Conv2D(uint32_t filters, nnom_shape_t k, nnom_shape_t s, nnom_padding_t pad, nnom_weight_t *w, nnom_bias_t *b); // depthwise_convolution 1D/2D nnom_layer_t* DW_Conv2D(uint32_t multiplier, nnom_shape_t k, nnom_shape_t s, nnom_padding_t pad, nnom_weight_t *w, nnom_bias_t *b); // fully connected, dense nnom_layer_t* Dense(size_t output_unit, nnom_weight_t *w, nnom_bias_t *b); // Lambda Layers // layer.run() , required // layer.oshape(), optional, call default_output_shape() if left NULL // layer.free() , optional, called while model is deleting, to free private resources // parameters , private parameters for run method, left NULL if not needed. nnom_layer_t *Lambda(nnom_status_t (*run)(nnom_layer_t *), nnom_status_t (*oshape)(nnom_layer_t *), nnom_status_t (*free)(nnom_layer_t *), void *parameters); About the missing Batch Normalization Layer Batch Normalization layer can be fused into the last convolution layer. So NNoM currently does not provide a Batch Normalization Layer. It might be implemented as a single layer in the future. However, currently, please fused it to the last layer. Further reading about fusing BN parameters to conv weights Fusing batch-norm layers Addictionlly, Activation APIs Actication APIs are not essential in the original idea. The original idea is making eveything as a layer. However, single layer instances cost huge amount of memories(100~150 Bytes), while activations are relativly simple, mostly have same input/output shape, a few/none parameter(s)... Therefore, to reduce the complexity, the \"actail\"(activation tail) is added to each layer instance. If a layer's Actail is not null, it will be called right after the layer is executed. Actail takes activation instance as input. The model API, model.active() will attach the activation to the layer's actail. // attach act to target_layer, return the target layer instance. nnom_layer_t * model.active(nnom_activation_t* act, nnom_layer_t * target_layer) The Activation APIs are listed in nnom_activations.h // Activation nnom_activation_t* act_relu(void); nnom_activation_t* act_sigmoid(void); nnom_activation_t* act_tanh(void); Model API A model instance contains the starting layer, the end layer and other neccessary info. Please refer to the examples for usage // Create or initial a new model() nnom_model_t* new_model(nnom_model_t* m); // Delete the model completely (new). void model_delete(nnom_model_t* m); // Compile a sequencial model. nnom_status_t sequencial_compile(nnom_model_t *m); // Compile a functional model with specified input layer and output layer. // if output = NULL, the output is automatic selected. nnom_status_t model_compile(nnom_model_t *m, nnom_layer_t* input, nnom_layer_t* output); // Run the model. nnom_status_t model_run(nnom_model_t *m); Known Issues Shared output buffer destroyed by single buffer layers (input-destructive) Single buffer layers (Such as most of the Activations, additionally MaxPool/AvgPool) are working directly on its input buffer. While its input buffer is shared with other parallel layers, and it is placed before other layers in a parallel structure (such as Inception), the shared buffer will be destroyed by those input-destructive before other parallel layer can access it. Additionally, although, MaxPool & AvgPool are not single buffer layers, they will destroy the input buffer as they are mentioned with input-destructive layers in CMSIS-NN. So they should be treated as same as single buffer layers. Fix plan of the issue Not planned. Possiblly, add an invisible copying layer/functions to copy data for single input layer before passing to other parallel layers. Current work around Work around 1 If the Inception has only one single buffer layer, always hook the single buffer layer at the end. For example, instead of doing MaxPool - Conv2D - Conv2D , do Conv2D - Conv2D - MaxPool // the codes are faked and simplified, please rewrite them according to corresponding APIs. // original x1 = model.hook(MaxPool(), input); // Single buffer layer, this will destroyed the buffer x2 = model.hook(Conv2D(), input); // buffer destroyed. x3 = model.hook(Conv2D(), input); // buffer destroyed. output = model.mergex(Concat(-1), 3, x1, x2, x3); // This will fixed the problem without affacting the concatenate order. // notice that the order of x1,x2,x3 will change, // the different is the order that the inception layers hooked to the input layer. x3 = model.hook(Conv2D(), input); // multiple buffers layer x2 = model.hook(Conv2D(), input); // x1 = model.hook(MaxPool(), input); // this will destroyed the buffer, but it doesnt matter now. output = model.mergex(Concat(-1), 3, x1, x2, x3); Work around 2 If there is multiple, add an extra multiple bufer layer before the single buffer layer. Such as using Lambda() layer to copy buffer. // the codes are faked and simplified, please rewrite them according to corresponding APIs. lambda_run(layer) { memcpy(layer->output, layer->input, sizeof(inputshape); } x1 = model.hook(Lambda(lambda_run), input); // add a lambda to copy data x1 = model.hook(MaxPool(), x1); // now it is only destroying Lambda's output buffer instead of the input layer's. x2 = model.hook(Lambda(lambda_run), input); // add a lambda to copy data x2 = model.hook(MaxPool(), x2); x3 = model.hook(Conv2D(), input); output = model.mergex(Concat(-1), 3, x1, x2, x3); Evaluation The evaluation methods are listed in nnom_utils.h They run the model with testing data, then evaluate the model. Includes Top-k accuracy, confusion matrix, runtime stat... Please refer to UCI HAR example for usage. // create a prediction // input model, the buf pointer to the softwmax output (Temporary, this can be extract from model) // the size of softmax output (the num of lable) // the top k that wants to record. nnom_predic_t* prediction_create(nnom_model_t* m, int8_t* buf_prediction, size_t label_num, size_t top_k_size);// currently int8_t // after a new data is set in input // feed data to prediction // input the current label, (range from 0 to total number of label -1) // (the current input data should be set by user manully to the input buffer of the model.) // return NN_ARGUMENT_ERROR if parameter error int32_t prediction_run(nnom_predic_t *pre, uint32_t label); // to mark prediction finished void prediction_end(nnom_predic_t *pre); // free all resources void predicetion_delete(nnom_predic_t *pre); // print matrix void prediction_matrix(nnom_predic_t *pre); // print top-k void prediction_top_k(nnom_predic_t *pre); // this function is to print sumarry void prediction_summary(nnom_predic_t *pre); // ------------------------------- // stand alone prediction API // this api test one set of data, return the prediction // input the model's input and output bufer // return the predicted label // return NN_ARGUMENT_ERROR if parameter error int32_t nnom_predic_one(nnom_model_t *m, int8_t *input, int8_t *output); // currently int8_t // print last runtime stat of the model void model_stat(nnom_model_t *m); Demo of Evaluation The UCI HAR example runs on RT-Thread, uses Y-Modem to receive testing dataset, uses ringbuffer to store data, and the console (msh) to print the results. The layer order, activation, output shape, operation, memory of I/O, and assigned memory block are shown. It also summarised the memory cost by neural network. Type predic , then use Y-Modem to send the data file. The model will run once enough data is received. When the file copying done, the runtime summary, Top-k and confusion matrix will be printed Optionally, the runtime stat detail of each layer can be printed by nn_stat PS: The \"runtime stat\" in the animation is not correct, due to the test chip is overclocking (STM32L476 @ 160MHz, 2x overclocking), and the timer is overclocking as well. However, the numbers in prediction summary are correct, because they are measured by system_tick timer which is not overclocking.","title":"A Temporary Guide"},{"location":"A Temporary Guide to NNoM/#deploying","text":"Deploying is much easier than before. (Thanks to @parai) Simply use generate_model(model, x_data) to generate a C header weights.h after you have trained your model in Keras. It is available in nnom_utils.py Include the weights.h in your project, then call nnom_model_create() to create and compile the model on the MCU. Finaly, call model_run() to do your prediction. Please check MNIST-DenseNet example for usage The generate_model(model, x_data) might not be updated with NNoM from time to time. For new features and customized layers, you can still use NNoM APIs to build your model.","title":"Deploying"},{"location":"A Temporary Guide to NNoM/#nnom-structure","text":"NNoM uses a layer-based structure. A layer is a container. Every operation (convolution, concat...) must be wrapped into a layer. A basic layer contains a list of Input/Ouput modules (I/O). Each of I/O contains a list of Hook (similar to Nodes in Keras). Hook stores the links to an I/O (other layer's) I/O is a buffer to store input/output data of the operation. Dont be scared, check this: Those APIs listed below will help you to create layers and build the model structures.","title":"NNoM Structure"},{"location":"A Temporary Guide to NNoM/#apis","text":"Layer APIs and Construction APIs are used to build a model. Layer APIs can create and return a new layer instance, while construction APIs use layer instances to build a model. Layer APIs such as Conv2D(), Dense(), Activation() ... which you can find in nnom_layers.h Construction APIs such as model.hook(), model.merge(), model.add() ... which you can find in new_model() at nnom.c For example, to add a convolution layer into a sequencial model, use model.add() : model.add(&model, Conv2D(16, kernel(1, 9), stride(1, 2), PADDING_SAME, &c1_w, &c1_b)); In functional model, the links between layer are specified explicitly by using model.hook() or model.merge() x = model.hook(Conv2D(16, kernel(1, 9), stride(1, 2), PADDING_SAME, &c1_w, &c1_b), input_layer); x = model.hook(MaxPool(kernel(1, 2), stride(1, 2), PADDING_VALID), x); NNoM currently supports HWC format. Which also called \"channel last\", where H = number of rows or y axis, W = number of column or x axis, C = number of channes. For example: In the above codes, both kernal(H, W) and stride(H, W) returns a 'shape' instance. The shape instance in format of (H, W, ?) All convolutional layers and poolings layers support both 1D / 2D data input. However, when using 1D input, the H must be set to 1.","title":"APIs"},{"location":"A Temporary Guide to NNoM/#construction-apis","text":"Construction APIs are statics functions located in nnom.c Currently are: Sequencial Construction API nnom_status_t model.add(nnom_model_t* model, nnom_layer_t *layer); Functional Construction API // hook the current layer to the input layer // this function only to connect (single output layer) to (single input layer). // return the curr (layer) instance nnom_layer_t * model.hook(nnom_layer_t* curr, nnom_layer_t *last) // merge 2 layer's output to one output by provided merging method(a mutiple input layer) // method = merging layer such as (concat(), dot(), mult(), add()) // return the method (layer) instance nnom_layer_t * model.merge(nnom_layer_t *method, nnom_layer_t *in1, nnom_layer_t *in2) // Same as model.merge() // Except it can take mutiple layers as input. // num = the number of layer // method: same as model.merge() nnom_layer_t * model.mergex(nnom_layer_t *method, int num, ...) // This api will merge the activation to the targeted layerto reduce an extra activation layer // activation such as (act_relu(), act_tanh()...) nnom_layer_t * model.active(nnom_activation_t* act, nnom_layer_t * target) For model.active() , please check Activation APIs below.","title":"Construction APIs"},{"location":"A Temporary Guide to NNoM/#layer-apis","text":"Layers APIs are listed in nnom_layers.h Input/output layers are neccessary for a model. They are responsible to copy data from user's input buffer, and copy out to user's output buffer. // Layer APIs // input/output nnom_layer_t* Input(nnom_shape_t input_shape\uff0cvoid* p_buf); nnom_layer_t* Output(nnom_shape_t output_shape, void* p_buf); Pooling as they are: The sum pooling here will dynamicly change its ourput shift to avoid overflowing. It is recommened to replace the Global Average Pooling by Global Sum Pooling for better accuracy in MCU side. // Pooling, kernel, strides, padding nnom_layer_t* MaxPool(nnom_shape_t k, nnom_shape_t s, nnom_padding_t pad); nnom_layer_t* AvgPool(nnom_shape_t k, nnom_shape_t s, nnom_padding_t pad); nnom_layer_t* SumPool(nnom_shape_t k, nnom_shape_t s, nnom_padding_t pad); // The Global poolings simplly do better nnom_layer_t *GlobalMaxPool(void); nnom_layer_t *GlobalAvgPool(void); nnom_layer_t *GlobalSumPool(void); // Upsampling layer / Unpooling layer nnom_layer_t* UpSample(nnom_shape_t kernel); Activation's Layers API are started with capital letter. They are differed from the Activation API , which start with act_* and retrun an activation instance. Pleas check the Activation APIs below for more detail. They return a layer instance. // Activation layers take activation instance as input. nnom_layer_t* Activation(nnom_activation_t *act); // Activation's layer API. nnom_layer_t* ReLU(void); nnom_layer_t* Softmax(void); nnom_layer_t* Sigmoid(void); nnom_layer_t* TanH(void); Matrix API. These layers normally take 2 or more layer's output as their inputs. They also called \"merging method\", which must be used by model.merge(method, in1, in2) or model.mergex(method, num of input, in1, in2, 1n3 ...) // Matrix nnom_layer_t* Add(void); nnom_layer_t* Sub(void); nnom_layer_t* Mult(void); nnom_layer_t* Concat(int8_t axis); Flatten change the shapes to (x, 1, 1) // utils nnom_layer_t* Flatten(void); Stable NN layers. For more developing layers, please check the source codes. // conv1D/2d nnom_layer_t* Conv2D(uint32_t filters, nnom_shape_t k, nnom_shape_t s, nnom_padding_t pad, nnom_weight_t *w, nnom_bias_t *b); // depthwise_convolution 1D/2D nnom_layer_t* DW_Conv2D(uint32_t multiplier, nnom_shape_t k, nnom_shape_t s, nnom_padding_t pad, nnom_weight_t *w, nnom_bias_t *b); // fully connected, dense nnom_layer_t* Dense(size_t output_unit, nnom_weight_t *w, nnom_bias_t *b); // Lambda Layers // layer.run() , required // layer.oshape(), optional, call default_output_shape() if left NULL // layer.free() , optional, called while model is deleting, to free private resources // parameters , private parameters for run method, left NULL if not needed. nnom_layer_t *Lambda(nnom_status_t (*run)(nnom_layer_t *), nnom_status_t (*oshape)(nnom_layer_t *), nnom_status_t (*free)(nnom_layer_t *), void *parameters); About the missing Batch Normalization Layer Batch Normalization layer can be fused into the last convolution layer. So NNoM currently does not provide a Batch Normalization Layer. It might be implemented as a single layer in the future. However, currently, please fused it to the last layer. Further reading about fusing BN parameters to conv weights Fusing batch-norm layers","title":"Layer APIs"},{"location":"A Temporary Guide to NNoM/#addictionlly-activation-apis","text":"Actication APIs are not essential in the original idea. The original idea is making eveything as a layer. However, single layer instances cost huge amount of memories(100~150 Bytes), while activations are relativly simple, mostly have same input/output shape, a few/none parameter(s)... Therefore, to reduce the complexity, the \"actail\"(activation tail) is added to each layer instance. If a layer's Actail is not null, it will be called right after the layer is executed. Actail takes activation instance as input. The model API, model.active() will attach the activation to the layer's actail. // attach act to target_layer, return the target layer instance. nnom_layer_t * model.active(nnom_activation_t* act, nnom_layer_t * target_layer) The Activation APIs are listed in nnom_activations.h // Activation nnom_activation_t* act_relu(void); nnom_activation_t* act_sigmoid(void); nnom_activation_t* act_tanh(void);","title":"Addictionlly, Activation APIs"},{"location":"A Temporary Guide to NNoM/#model-api","text":"A model instance contains the starting layer, the end layer and other neccessary info. Please refer to the examples for usage // Create or initial a new model() nnom_model_t* new_model(nnom_model_t* m); // Delete the model completely (new). void model_delete(nnom_model_t* m); // Compile a sequencial model. nnom_status_t sequencial_compile(nnom_model_t *m); // Compile a functional model with specified input layer and output layer. // if output = NULL, the output is automatic selected. nnom_status_t model_compile(nnom_model_t *m, nnom_layer_t* input, nnom_layer_t* output); // Run the model. nnom_status_t model_run(nnom_model_t *m);","title":"Model API"},{"location":"A Temporary Guide to NNoM/#known-issues","text":"","title":"Known Issues"},{"location":"A Temporary Guide to NNoM/#shared-output-buffer-destroyed-by-single-buffer-layers-input-destructive","text":"Single buffer layers (Such as most of the Activations, additionally MaxPool/AvgPool) are working directly on its input buffer. While its input buffer is shared with other parallel layers, and it is placed before other layers in a parallel structure (such as Inception), the shared buffer will be destroyed by those input-destructive before other parallel layer can access it. Additionally, although, MaxPool & AvgPool are not single buffer layers, they will destroy the input buffer as they are mentioned with input-destructive layers in CMSIS-NN. So they should be treated as same as single buffer layers. Fix plan of the issue Not planned. Possiblly, add an invisible copying layer/functions to copy data for single input layer before passing to other parallel layers. Current work around Work around 1 If the Inception has only one single buffer layer, always hook the single buffer layer at the end. For example, instead of doing MaxPool - Conv2D - Conv2D , do Conv2D - Conv2D - MaxPool // the codes are faked and simplified, please rewrite them according to corresponding APIs. // original x1 = model.hook(MaxPool(), input); // Single buffer layer, this will destroyed the buffer x2 = model.hook(Conv2D(), input); // buffer destroyed. x3 = model.hook(Conv2D(), input); // buffer destroyed. output = model.mergex(Concat(-1), 3, x1, x2, x3); // This will fixed the problem without affacting the concatenate order. // notice that the order of x1,x2,x3 will change, // the different is the order that the inception layers hooked to the input layer. x3 = model.hook(Conv2D(), input); // multiple buffers layer x2 = model.hook(Conv2D(), input); // x1 = model.hook(MaxPool(), input); // this will destroyed the buffer, but it doesnt matter now. output = model.mergex(Concat(-1), 3, x1, x2, x3); Work around 2 If there is multiple, add an extra multiple bufer layer before the single buffer layer. Such as using Lambda() layer to copy buffer. // the codes are faked and simplified, please rewrite them according to corresponding APIs. lambda_run(layer) { memcpy(layer->output, layer->input, sizeof(inputshape); } x1 = model.hook(Lambda(lambda_run), input); // add a lambda to copy data x1 = model.hook(MaxPool(), x1); // now it is only destroying Lambda's output buffer instead of the input layer's. x2 = model.hook(Lambda(lambda_run), input); // add a lambda to copy data x2 = model.hook(MaxPool(), x2); x3 = model.hook(Conv2D(), input); output = model.mergex(Concat(-1), 3, x1, x2, x3);","title":"Shared output buffer destroyed by single buffer layers (input-destructive)"},{"location":"A Temporary Guide to NNoM/#evaluation","text":"The evaluation methods are listed in nnom_utils.h They run the model with testing data, then evaluate the model. Includes Top-k accuracy, confusion matrix, runtime stat... Please refer to UCI HAR example for usage. // create a prediction // input model, the buf pointer to the softwmax output (Temporary, this can be extract from model) // the size of softmax output (the num of lable) // the top k that wants to record. nnom_predic_t* prediction_create(nnom_model_t* m, int8_t* buf_prediction, size_t label_num, size_t top_k_size);// currently int8_t // after a new data is set in input // feed data to prediction // input the current label, (range from 0 to total number of label -1) // (the current input data should be set by user manully to the input buffer of the model.) // return NN_ARGUMENT_ERROR if parameter error int32_t prediction_run(nnom_predic_t *pre, uint32_t label); // to mark prediction finished void prediction_end(nnom_predic_t *pre); // free all resources void predicetion_delete(nnom_predic_t *pre); // print matrix void prediction_matrix(nnom_predic_t *pre); // print top-k void prediction_top_k(nnom_predic_t *pre); // this function is to print sumarry void prediction_summary(nnom_predic_t *pre); // ------------------------------- // stand alone prediction API // this api test one set of data, return the prediction // input the model's input and output bufer // return the predicted label // return NN_ARGUMENT_ERROR if parameter error int32_t nnom_predic_one(nnom_model_t *m, int8_t *input, int8_t *output); // currently int8_t // print last runtime stat of the model void model_stat(nnom_model_t *m);","title":"Evaluation"},{"location":"A Temporary Guide to NNoM/#demo-of-evaluation","text":"The UCI HAR example runs on RT-Thread, uses Y-Modem to receive testing dataset, uses ringbuffer to store data, and the console (msh) to print the results. The layer order, activation, output shape, operation, memory of I/O, and assigned memory block are shown. It also summarised the memory cost by neural network. Type predic , then use Y-Modem to send the data file. The model will run once enough data is received. When the file copying done, the runtime summary, Top-k and confusion matrix will be printed Optionally, the runtime stat detail of each layer can be printed by nn_stat PS: The \"runtime stat\" in the animation is not correct, due to the test chip is overclocking (STM32L476 @ 160MHz, 2x overclocking), and the timer is overclocking as well. However, the numbers in prediction summary are correct, because they are measured by system_tick timer which is not overclocking.","title":"Demo of Evaluation"},{"location":"Porting and Optimisation Guide/","text":"Porting Porting is not necessary since NNoM is a pure C framework. It will run without any problem by default setting. However, porting can gain better performance in some platforms by switch the backends or to provide print-out model info and evaluation. Options Porting is simply done by modified the nnom_port.h under port/ The default setting is shown below. // memory interfaces #define nnom_malloc(n) malloc(n) #define nnom_free(p) free(p) #define nnom_memset(p,v,s) memset(p,v,s) // runtime & debuges #define nnom_us_get() 0 #define nnom_ms_get() 0 #define LOG(...) printf(__VA_ARGS__) // NNoM configuration #define NNOM_BLOCK_NUM (8) // maximum number of memory block #define DENSE_WEIGHT_OPT (1) // if used fully connected layer optimized weights. //#define NNOM_USING_CMSIS_NN // uncomment if use CMSIS-NN for optimation Memory interfaces Memory interfaces are required. If your platform doesnt support std interface, please modify them according to your platform. Runtime & debuges They are optional. Its recommented to port them if your platform has console or terminal. They will help you to validate your model. nnom_us_get() is used in runtime analysis. If presented, NNoM would be able to record the time cost for each layer and calcualte the effeciency of them. This method should return the current time in a us resolution (16/32-bit unsigned value, values can overflow). nnom_ms_get() is used in evaluation with APIs in 'nnom_utils.c'. It is used to evaluate the performance with the whole banch of testing data. LOG() is used to print model compiling info and evaluation info. NNoM configuration NNOM_BLOCK_NUM is the maximum number of memory block. The utilisation of memory block will be printed during compiling. Adjust it when needed. DENSE_WEIGHT_OPT , reorder weights for dense will gain better performance. If your model is using 'nnom_utils.py' to deploy, weights are already reordered. CMSIS-NN backend On ARM-Cortex-M4/7/33/35P chips, the performance can be increased about 5x while you enable it. For detail please check the paper To switch the backend from local backend to the optimized CMSIS-NN/DSP, simply uncomment the line #define NNOM_USING_CMSIS_NN . Then, in your project, you must: 1. Include the CMSIS-NN as well as CMSIS-DSP in your project. 2. Make sure the optimisation is enable on CMSIS-NN/DSP. Notess It is required that CMSIS version above 5.5.1+ (NN version > 1.1.0, DSP version 1.6.0). Make sure your compiler is using the new version of \"arm_math.h\". There might be a few duplicated in a project, such as the STM32 HAL has its own version of \"arm_math.h\" You might also define your chip core and enable your FPU support in your pre-compile configuration if you are not able to compile. i.e. when using STM32L476, you might add the two macro in your project ' ARM_MATH_CM4, __FPU_PRESENT=1' After all, you can try to evaluate the performance using the APIs in 'nnom_utils.c' Examples Porting for RT-Thread // memory interfaces #define nnom_malloc(n) malloc(n) #define nnom_free(p) free(p) #define nnom_memset(p,v,s) memset(p,v,s) // runtime & debuges #define nnom_us_get() 0 #define nnom_ms_get() rt_tick_get() // when tick is set to 1000 #define LOG(...) rt_kprintf(__VA_ARGS__) // NNoM configuration #define NNOM_BLOCK_NUM (8) // maximum number of memory block #define DENSE_WEIGHT_OPT (1) // if used fully connected layer optimized weights. //#define NNOM_USING_CMSIS_NN // uncomment if use CMSIS-NN for optimation","title":"Porting and Optimisation Guide"},{"location":"Porting and Optimisation Guide/#porting","text":"Porting is not necessary since NNoM is a pure C framework. It will run without any problem by default setting. However, porting can gain better performance in some platforms by switch the backends or to provide print-out model info and evaluation.","title":"Porting"},{"location":"Porting and Optimisation Guide/#options","text":"Porting is simply done by modified the nnom_port.h under port/ The default setting is shown below. // memory interfaces #define nnom_malloc(n) malloc(n) #define nnom_free(p) free(p) #define nnom_memset(p,v,s) memset(p,v,s) // runtime & debuges #define nnom_us_get() 0 #define nnom_ms_get() 0 #define LOG(...) printf(__VA_ARGS__) // NNoM configuration #define NNOM_BLOCK_NUM (8) // maximum number of memory block #define DENSE_WEIGHT_OPT (1) // if used fully connected layer optimized weights. //#define NNOM_USING_CMSIS_NN // uncomment if use CMSIS-NN for optimation","title":"Options"},{"location":"Porting and Optimisation Guide/#memory-interfaces","text":"Memory interfaces are required. If your platform doesnt support std interface, please modify them according to your platform.","title":"Memory interfaces"},{"location":"Porting and Optimisation Guide/#runtime-debuges","text":"They are optional. Its recommented to port them if your platform has console or terminal. They will help you to validate your model. nnom_us_get() is used in runtime analysis. If presented, NNoM would be able to record the time cost for each layer and calcualte the effeciency of them. This method should return the current time in a us resolution (16/32-bit unsigned value, values can overflow). nnom_ms_get() is used in evaluation with APIs in 'nnom_utils.c'. It is used to evaluate the performance with the whole banch of testing data. LOG() is used to print model compiling info and evaluation info.","title":"Runtime &amp; debuges"},{"location":"Porting and Optimisation Guide/#nnom-configuration","text":"NNOM_BLOCK_NUM is the maximum number of memory block. The utilisation of memory block will be printed during compiling. Adjust it when needed. DENSE_WEIGHT_OPT , reorder weights for dense will gain better performance. If your model is using 'nnom_utils.py' to deploy, weights are already reordered.","title":"NNoM configuration"},{"location":"Porting and Optimisation Guide/#cmsis-nn-backend","text":"On ARM-Cortex-M4/7/33/35P chips, the performance can be increased about 5x while you enable it. For detail please check the paper To switch the backend from local backend to the optimized CMSIS-NN/DSP, simply uncomment the line #define NNOM_USING_CMSIS_NN . Then, in your project, you must: 1. Include the CMSIS-NN as well as CMSIS-DSP in your project. 2. Make sure the optimisation is enable on CMSIS-NN/DSP. Notess It is required that CMSIS version above 5.5.1+ (NN version > 1.1.0, DSP version 1.6.0). Make sure your compiler is using the new version of \"arm_math.h\". There might be a few duplicated in a project, such as the STM32 HAL has its own version of \"arm_math.h\" You might also define your chip core and enable your FPU support in your pre-compile configuration if you are not able to compile. i.e. when using STM32L476, you might add the two macro in your project ' ARM_MATH_CM4, __FPU_PRESENT=1' After all, you can try to evaluate the performance using the APIs in 'nnom_utils.c'","title":"CMSIS-NN backend"},{"location":"Porting and Optimisation Guide/#examples","text":"","title":"Examples"},{"location":"Porting and Optimisation Guide/#porting-for-rt-thread","text":"// memory interfaces #define nnom_malloc(n) malloc(n) #define nnom_free(p) free(p) #define nnom_memset(p,v,s) memset(p,v,s) // runtime & debuges #define nnom_us_get() 0 #define nnom_ms_get() rt_tick_get() // when tick is set to 1000 #define LOG(...) rt_kprintf(__VA_ARGS__) // NNoM configuration #define NNOM_BLOCK_NUM (8) // maximum number of memory block #define DENSE_WEIGHT_OPT (1) // if used fully connected layer optimized weights. //#define NNOM_USING_CMSIS_NN // uncomment if use CMSIS-NN for optimation","title":"Porting for RT-Thread"},{"location":"api_activations/","text":"Activation Layers To reduce the memory footprint, activations provides both Layer APIs and Activation APIs . Layer APIs will create a layer instance for running the activation; Activation APIs will only create activation instance, which can be attached on a existing layers. Activation nnom_layer_t* Activation(nnom_activation_t *act); This is the Layer API wrapper for activations. It take activation instance as input. Arguments act: is the activation instance. Return The activation layer instance Softmax nnom_layer_t* Softmax(void); Return The Softmax layer instance Notes Softmax only has Layer API. ReLU (Layer API) nnom_layer_t* ReLU(void); Return The ReLU layer instance Notes Using layer = ReLU(); is no difference to layer = Activation(act_relu()); Sigmoid (Layer API) This function is affacted by an issue that we are currently working on. Check issue nnom_layer_t* Sigmoid(void); Return The Sigmoid layer instance Notes Using layer = Sigmoid(); is no difference to layer = Activation(act_sigmoid()); TanH (Layer API) nnom_layer_t* TanH(void); Return The TanH layer instance This function is affacted by an issue that we are currently working on. Check issue Notes Using layer = TanH(); is no difference to layer = Activation(act_tanh()); Activation APIs nnom_activation_t* act_relu(void); nnom_activation_t* act_sigmoid(void); nnom_activation_t* act_tanh(void); They return the activation instance which can be passed to either model.active() or Activation() Examples Using Layer's API Activation's layer API allows you to us them as a layer. nnom_layer_t layer; model.add(&model, Dense(10)); model.add(&model, ReLU()); nnom_layer_t layer; model.add(&model, Dense(10)); model.add(&model, Activation(act_relu())); nnom_layer_t layer; input = Input(shape(1, 10, 1), buffer); layer = model.hook(Dense(10), input); layer = model.hook(ReLU(), layer); All 3 above perform the same and take the same memory. Using Activation's API nnom_layer_t layer; input = Input(shape(1, 10, 1), buffer); layer = model.hook(Dense(10), input); layer = model.active(act_relu(), layer); This method perform the same but take less memory due to it uses the activation directly.","title":"Activations"},{"location":"api_activations/#activation-layers","text":"To reduce the memory footprint, activations provides both Layer APIs and Activation APIs . Layer APIs will create a layer instance for running the activation; Activation APIs will only create activation instance, which can be attached on a existing layers.","title":"Activation Layers"},{"location":"api_activations/#activation","text":"nnom_layer_t* Activation(nnom_activation_t *act); This is the Layer API wrapper for activations. It take activation instance as input. Arguments act: is the activation instance. Return The activation layer instance","title":"Activation"},{"location":"api_activations/#softmax","text":"nnom_layer_t* Softmax(void); Return The Softmax layer instance Notes Softmax only has Layer API.","title":"Softmax"},{"location":"api_activations/#relu-layer-api","text":"nnom_layer_t* ReLU(void); Return The ReLU layer instance Notes Using layer = ReLU(); is no difference to layer = Activation(act_relu());","title":"ReLU (Layer API)"},{"location":"api_activations/#sigmoid-layer-api","text":"This function is affacted by an issue that we are currently working on. Check issue nnom_layer_t* Sigmoid(void); Return The Sigmoid layer instance Notes Using layer = Sigmoid(); is no difference to layer = Activation(act_sigmoid());","title":"Sigmoid (Layer API)"},{"location":"api_activations/#tanh-layer-api","text":"nnom_layer_t* TanH(void); Return The TanH layer instance This function is affacted by an issue that we are currently working on. Check issue Notes Using layer = TanH(); is no difference to layer = Activation(act_tanh());","title":"TanH (Layer API)"},{"location":"api_activations/#activation-apis","text":"nnom_activation_t* act_relu(void); nnom_activation_t* act_sigmoid(void); nnom_activation_t* act_tanh(void); They return the activation instance which can be passed to either model.active() or Activation()","title":"Activation APIs"},{"location":"api_activations/#examples","text":"Using Layer's API Activation's layer API allows you to us them as a layer. nnom_layer_t layer; model.add(&model, Dense(10)); model.add(&model, ReLU()); nnom_layer_t layer; model.add(&model, Dense(10)); model.add(&model, Activation(act_relu())); nnom_layer_t layer; input = Input(shape(1, 10, 1), buffer); layer = model.hook(Dense(10), input); layer = model.hook(ReLU(), layer); All 3 above perform the same and take the same memory. Using Activation's API nnom_layer_t layer; input = Input(shape(1, 10, 1), buffer); layer = model.hook(Dense(10), input); layer = model.active(act_relu(), layer); This method perform the same but take less memory due to it uses the activation directly.","title":"Examples"},{"location":"api_construction/","text":"Constructions APIs NNoM support Sequential model and Functional model similar to Keras. NNoM treat them equaly in compiling and running, the only difference the methods of constructions. In Sequential model , the layer are stacked one by one sequently using model.add() In Functional model , the links between layer are specified explicitly by using model.hook() or model.merge() model.add nnom_status_t model.add(nnom_model_t *model, nnom_layer_t *layer); The only sequencial constructor. It stack the new layer instance to the existing model. Arguments model: the target model for stacking new. layer: the new layer instance for stacking onto the model. Return The state of the operation. Example to stack 2 layers on a model model.add(&model, Conv2D(16, kernel(1, 9), stride(1, 2), PADDING_SAME, &c1_w, &c1_b)); model.add(&model, ReLU()); ... Notes You can stack like this whenever there are memory available :-) model.hook nnom_layer_t *model.hook(nnom_layer_t *curr, nnom_layer_t *last); A functional constructor to explicitly hook two layers togethers. When two layers are hook, the previous layer's output (last) will be the input of the new later (curr). Arguments curr: the new layer for hooking to a previous built layer. last: the previous built layer instance. Return The curr layer instance. Note A layer instance can be hooked many times (act as \"last\" layer). NNoM will manage the run order of them. This is very useful when many layers wants to take the same output of previous layer. The example is many towers layer share one output in Inception structure. model.merge nnom_layer_t *model.merge(nnom_layer_t *method, nnom_layer_t *in1, nnom_layer_t *in2); A functional constructor for merge many layers' output by the specified merging methods(layer). Specificaly, this method merge 2 layer's output. Arguments method: the merging layer method. One of Concat(), Mult(), Add(), Sub() in1: the first layer instance to merge. in2: the second layer instance to merge. Return The method (layer) instance. model.mergex nnom_layer_t *model.mergex(nnom_layer_t *method, int num, ...) A functional constructor for merge many layers' output by the specified merging methods(layer). Arguments method: the merging layer method. One of Concat(), Mult(), Add(), Sub() num: number of layer that needs to be merged. ...: the list of merge layer instances. Return The method (layer) instance. Note Currently, all \"merge methods\" support mutiple input layers, they will be processed one by one with the order provided by the list. model.active nnom_layer_t *model.active(nnom_activation_t *act, nnom_layer_t *target) A functional constructor, it merges the activation to the targeted layer to avoid an redundant activation layer, which costs more memory. Arguments act: the activation instance, please check Activation for more detail. target: the layer which output will be activated by the provided activation. Return The targeted (layer) instance. Examples This example shows the construction of an Inception model. nnom_layer_t* input_layer, *x, *x1, *x2, *x3; input_layer = Input(shape(INPUT_HIGHT, INPUT_WIDTH, INPUT_CH), nnom_input_data); // conv2d - 1 - inception x1 = model.hook(Conv2D(16, kernel(1, 5), stride(1, 1), PADDING_SAME, &c2_w, &c2_b), x); x1 = model.active(act_relu(), x1); x1 = model.hook(MaxPool(kernel(1, 2), stride(1, 2), PADDING_VALID), x1); // conv2d - 2 - inception x2 = model.hook(Conv2D(16, kernel(1, 3), stride(1, 1), PADDING_SAME, &c3_w, &c3_b), x); x2 = model.active(act_relu(), x2); x2 = model.hook(MaxPool(kernel(1, 2), stride(1, 2), PADDING_VALID), x2); // maxpool - 3 - inception x3 = model.hook(MaxPool(kernel(1, 2), stride(1, 2), PADDING_VALID), x); // concatenate x = model.mergex(Concat(-1), 3, x1, x2, x3); // flatten x = model.hook(Flatten(), x); ...","title":"Construction"},{"location":"api_construction/#constructions-apis","text":"NNoM support Sequential model and Functional model similar to Keras. NNoM treat them equaly in compiling and running, the only difference the methods of constructions. In Sequential model , the layer are stacked one by one sequently using model.add() In Functional model , the links between layer are specified explicitly by using model.hook() or model.merge()","title":"Constructions APIs"},{"location":"api_construction/#modeladd","text":"nnom_status_t model.add(nnom_model_t *model, nnom_layer_t *layer); The only sequencial constructor. It stack the new layer instance to the existing model. Arguments model: the target model for stacking new. layer: the new layer instance for stacking onto the model. Return The state of the operation. Example to stack 2 layers on a model model.add(&model, Conv2D(16, kernel(1, 9), stride(1, 2), PADDING_SAME, &c1_w, &c1_b)); model.add(&model, ReLU()); ... Notes You can stack like this whenever there are memory available :-)","title":"model.add"},{"location":"api_construction/#modelhook","text":"nnom_layer_t *model.hook(nnom_layer_t *curr, nnom_layer_t *last); A functional constructor to explicitly hook two layers togethers. When two layers are hook, the previous layer's output (last) will be the input of the new later (curr). Arguments curr: the new layer for hooking to a previous built layer. last: the previous built layer instance. Return The curr layer instance. Note A layer instance can be hooked many times (act as \"last\" layer). NNoM will manage the run order of them. This is very useful when many layers wants to take the same output of previous layer. The example is many towers layer share one output in Inception structure.","title":"model.hook"},{"location":"api_construction/#modelmerge","text":"nnom_layer_t *model.merge(nnom_layer_t *method, nnom_layer_t *in1, nnom_layer_t *in2); A functional constructor for merge many layers' output by the specified merging methods(layer). Specificaly, this method merge 2 layer's output. Arguments method: the merging layer method. One of Concat(), Mult(), Add(), Sub() in1: the first layer instance to merge. in2: the second layer instance to merge. Return The method (layer) instance.","title":"model.merge"},{"location":"api_construction/#modelmergex","text":"nnom_layer_t *model.mergex(nnom_layer_t *method, int num, ...) A functional constructor for merge many layers' output by the specified merging methods(layer). Arguments method: the merging layer method. One of Concat(), Mult(), Add(), Sub() num: number of layer that needs to be merged. ...: the list of merge layer instances. Return The method (layer) instance. Note Currently, all \"merge methods\" support mutiple input layers, they will be processed one by one with the order provided by the list.","title":"model.mergex"},{"location":"api_construction/#modelactive","text":"nnom_layer_t *model.active(nnom_activation_t *act, nnom_layer_t *target) A functional constructor, it merges the activation to the targeted layer to avoid an redundant activation layer, which costs more memory. Arguments act: the activation instance, please check Activation for more detail. target: the layer which output will be activated by the provided activation. Return The targeted (layer) instance.","title":"model.active"},{"location":"api_construction/#examples","text":"This example shows the construction of an Inception model. nnom_layer_t* input_layer, *x, *x1, *x2, *x3; input_layer = Input(shape(INPUT_HIGHT, INPUT_WIDTH, INPUT_CH), nnom_input_data); // conv2d - 1 - inception x1 = model.hook(Conv2D(16, kernel(1, 5), stride(1, 1), PADDING_SAME, &c2_w, &c2_b), x); x1 = model.active(act_relu(), x1); x1 = model.hook(MaxPool(kernel(1, 2), stride(1, 2), PADDING_VALID), x1); // conv2d - 2 - inception x2 = model.hook(Conv2D(16, kernel(1, 3), stride(1, 1), PADDING_SAME, &c3_w, &c3_b), x); x2 = model.active(act_relu(), x2); x2 = model.hook(MaxPool(kernel(1, 2), stride(1, 2), PADDING_VALID), x2); // maxpool - 3 - inception x3 = model.hook(MaxPool(kernel(1, 2), stride(1, 2), PADDING_VALID), x); // concatenate x = model.mergex(Concat(-1), 3, x1, x2, x3); // flatten x = model.hook(Flatten(), x); ...","title":"Examples"},{"location":"api_evaluation/","text":"Evaluation tools NNoM has provide a few evaluation interfaces. Thye can either do runtime statistic or model evaluations. These API are print though the standard print() , thus a terminal/console is needed. All these API must not be call before the dedicated model has been compiled. model_stat void model_stat(nnom_model_t *m); To print runtime statistic of the last run. Check the below example for the availble statistics. Arguments m: the model to print. Notes It is recommended to run the mode once after compiling to gain these runtime statistic. Example Print running stat.. Layer(#) - Time(us) ops(MACs) ops/us -------------------------------------------------------- #1 Input - 9 0 #2 Conv2D - 8292 172800 20.83 #3 MaxPool - 5753 0 #4 Conv2D - 50095 3612672 72.11 #5 MaxPool - 3617 0 #6 Conv2D - 35893 2654208 73.94 #7 MaxPool - 1206 0 #8 Conv2D - 4412 171072 38.77 #9 GL_SumPool - 30 0 #10 Softmax - 2 0 #11 Output - 0 0 Summary. Total ops (MAC): 6610752 Prediction time :109309us Efficiency 60.47 ops/us Total Memory cost (Network and NNoM): 32876 nnom_predic_one int32_t nnom_predic_one(nnom_model_t *m, int8_t *input, int8_t *output); To predict one set of input data. Arguments m: the model to run prediction (evaluation). input: the data to predict. Can pass the buffer which has passed to Input Layer. output: the data to predict. Can pass the buffer which has passed to Output Layer. Return The predicted label in digit. prediction_create nnom_predic_t *prediction_create(nnom_model_t *m, int8_t *buf_prediction, size_t label_num, size_t top_k_size); This method create a prediction instance, which record mutiple parameters in the evaluation process. Arguments m: the model to run prediction (evaluation). buf_prediction: the output buffer of the model, which should be the output of Softmax. Size equal to the size of class. label_num: the number of labels (the number of classifications). top_k_size: the Top-k that wants to evaluate. Return - The prediction instance. Note Check later examples. prediction_run int32_t prediction_run(nnom_predic_t *pre, uint32_t label); To run a prodiction with the new data (feed by user to the input_buffer which passed to Input layer). Arguments pre: the prediction instance created by prediction_create() . label: the true label of this data. Return - The top-1 prediction of current data. prediction_end void prediction_end(nnom_predic_t *pre); To mark the prediction has done. Arguments pre: the prediction instance created by prediction_create() . prediction_delete void predicetion_delete(nnom_predic_t *pre); To free all resources. Arguments pre: the prediction instance created by prediction_create() . prediction_matrix void prediction_matrix(nnom_predic_t *pre); To print a confusion matrix when the prediction is done. Arguments pre: the prediction instance created by prediction_create() . Example Confusion matrix: predic 0 1 2 3 4 5 6 7 8 9 10 actual 0 | 395 1 0 0 2 0 0 0 0 0 21 | 94% 1 | 0 355 4 7 1 0 0 0 0 3 35 | 87% 2 | 0 3 325 2 1 0 7 29 3 2 53 | 76% 3 | 0 33 1 335 1 0 0 0 0 2 34 | 82% 4 | 6 0 1 0 371 3 0 0 0 0 31 | 90% 5 | 0 0 2 0 6 347 0 0 0 0 41 | 87% 6 | 0 1 5 8 0 0 322 4 0 0 56 | 81% 7 | 0 3 23 0 3 0 9 330 1 1 32 | 82% 8 | 0 0 5 2 0 0 0 0 343 4 57 | 83% 9 | 0 40 4 10 1 0 0 0 0 304 43 | 75% 10 | 4 61 16 34 28 17 14 6 12 37 6702 | 96% prediction_top_k void prediction_top_k(nnom_predic_t *pre); To print a Top-k when the prediction is done. Arguments pre: the prediction instance created by prediction_create() . Example Top 1 Accuracy: 92.03% Top 2 Accuracy: 96.39% Top 3 Accuracy: 97.38% Top 4 Accuracy: 97.85% Top 5 Accuracy: 98.13% Top 6 Accuracy: 98.40% Top 7 Accuracy: 98.59% Top 8 Accuracy: 98.88% Top 9 Accuracy: 99.14% Top 10 Accuracy: 99.60% prediction_summary void prediction_summary(nnom_predic_t *pre); To print a summary when the prediction is done. Arguments pre: the prediction instance created by prediction_create() . Example Prediction summary: Test frames: 11005 Test running time: 1598 sec Model running time: 1364908 ms Average prediction time: 124026 us Average effeciency: 53.30 ops/us Average frame rate: 8.0 Hz Example How to evaluate After a model has been compiled, then it is ready to be evaluated. The evaluation gose through a few steps Create a instance using prediction_create() Feed data one by one to the input buffer, then call prediction_run(pre) with true label. When all data has predicted, call prediction_end() . Then you can use prediction_matrix() , prediction_top_k() , and prediction_summary() to see the results. In addition, you can call model_stat() to see the performance of the last prediction. After all, call prediction_delete() to release all memory. How to implement in real-life Please check the UCI_HAR example for coding detail with RT-Thread and Y-modem. msh > \\ | / - RT - Thread Operating System / | \\ 4.0.0 build Mar 28 2019 2006 - 2018 Copyright by rt-thread team RTT Control Block Detection Address is 0x20000b3c msh > INFO: Start compile... Layer Activation output shape ops memory mem life-time ---------------------------------------------------------------------------------------------- Input - - ( 62, 12, 1) 0 ( 744, 744, 0) 1 - - - - - - - Conv2D - ReLU - ( 60, 10, 32) 172800 ( 744,19200, 1152) 1 1 - - - - - - MaxPool - - ( 30, 9, 32) 0 (19200, 8640, 0) 1 - 1 - - - - - Conv2D - ReLU - ( 28, 7, 64) 3612672 ( 8640,12544, 2304) 1 1 - - - - - - MaxPool - - ( 14, 6, 64) 0 (12544, 5376, 0) 1 - 1 - - - - - Conv2D - ReLU - ( 12, 4, 96) 2654208 ( 5376, 4608, 3456) 1 1 - - - - - - MaxPool - - ( 6, 3, 96) 0 ( 4608, 1728, 0) 1 - 1 - - - - - Conv2D - - ( 6, 3, 11) 171072 ( 1728, 198, 396) 1 1 - - - - - - GL_SumPool - - ( 1, 1, 11) 0 ( 198, 11, 44) 1 - 1 - - - - - Softmax - - ( 1, 1, 11) 0 ( 11, 11, 0) - 1 - - - - - - Output - - ( 11, 1, 1) 0 ( 11, 11, 0) 1 - - - - - - - ---------------------------------------------------------------------------------------------- INFO: memory analysis result Block0: 3456 Block1: 8640 Block2: 19200 Block3: 0 Block4: 0 Block5: 0 Block6: 0 Block7: 0 Total memory cost by network buffers: 31296 bytes msh >nn nn_stat msh >nn_stat Print running stat.. Layer(#) - Time(us) ops(MACs) ops/us -------------------------------------------------------- #1 Input - 9 0 #2 Conv2D - 8294 172800 20.83 #3 MaxPool - 5750 0 #4 Conv2D - 50089 3612672 72.12 #5 MaxPool - 3619 0 #6 Conv2D - 35890 2654208 73.95 #7 MaxPool - 1204 0 #8 Conv2D - 4411 171072 38.78 #9 GL_SumPool - 30 0 #10 Softmax - 3 0 #11 Output - 0 0 Summary. Total ops (MAC): 6610752 Prediction time :109299us Efficiency 60.48 ops/us Total Memory cost (Network and NNoM): 32876 msh >pre predic msh >predic Please select the NNoM binary test file and use Ymodem-128/1024 to send. CCCC Prediction done. Prediction summary: Test frames: 11005 Test running time: 1598 sec Model running time: 1364908 ms Average prediction time: 124026 us Average effeciency: 53.30 ops/us Average frame rate: 8.0 Hz Top 1 Accuracy: 92.03% Top 2 Accuracy: 96.39% Top 3 Accuracy: 97.38% Top 4 Accuracy: 97.85% Top 5 Accuracy: 98.13% Top 6 Accuracy: 98.40% Top 7 Accuracy: 98.59% Top 8 Accuracy: 98.88% Top 9 Accuracy: 99.14% Top 10 Accuracy: 99.60% Confusion matrix: predic 0 1 2 3 4 5 6 7 8 9 10 actual 0 | 395 1 0 0 2 0 0 0 0 0 21 | 94% 1 | 0 355 4 7 1 0 0 0 0 3 35 | 87% 2 | 0 3 325 2 1 0 7 29 3 2 53 | 76% 3 | 0 33 1 335 1 0 0 0 0 2 34 | 82% 4 | 6 0 1 0 371 3 0 0 0 0 31 | 90% 5 | 0 0 2 0 6 347 0 0 0 0 41 | 87% 6 | 0 1 5 8 0 0 322 4 0 0 56 | 81% 7 | 0 3 23 0 3 0 9 330 1 1 32 | 82% 8 | 0 0 5 2 0 0 0 0 343 4 57 | 83% 9 | 0 40 4 10 1 0 0 0 0 304 43 | 75% 10 | 4 61 16 34 28 17 14 6 12 37 6702 | 96% msh > OO: command not found. msh > msh >nn nn_stat msh >nn_stat Print running stat.. Layer(#) - Time(us) ops(MACs) ops/us -------------------------------------------------------- #1 Input - 9 0 #2 Conv2D - 8292 172800 20.83 #3 MaxPool - 5753 0 #4 Conv2D - 50095 3612672 72.11 #5 MaxPool - 3617 0 #6 Conv2D - 35893 2654208 73.94 #7 MaxPool - 1206 0 #8 Conv2D - 4412 171072 38.77 #9 GL_SumPool - 30 0 #10 Softmax - 2 0 #11 Output - 0 0 Summary. Total ops (MAC): 6610752 Prediction time :109309us Efficiency 60.47 ops/us Total Memory cost (Network and NNoM): 32876 msh >","title":"Evaluation"},{"location":"api_evaluation/#evaluation-tools","text":"NNoM has provide a few evaluation interfaces. Thye can either do runtime statistic or model evaluations. These API are print though the standard print() , thus a terminal/console is needed. All these API must not be call before the dedicated model has been compiled.","title":"Evaluation tools"},{"location":"api_evaluation/#model_stat","text":"void model_stat(nnom_model_t *m); To print runtime statistic of the last run. Check the below example for the availble statistics. Arguments m: the model to print. Notes It is recommended to run the mode once after compiling to gain these runtime statistic. Example Print running stat.. Layer(#) - Time(us) ops(MACs) ops/us -------------------------------------------------------- #1 Input - 9 0 #2 Conv2D - 8292 172800 20.83 #3 MaxPool - 5753 0 #4 Conv2D - 50095 3612672 72.11 #5 MaxPool - 3617 0 #6 Conv2D - 35893 2654208 73.94 #7 MaxPool - 1206 0 #8 Conv2D - 4412 171072 38.77 #9 GL_SumPool - 30 0 #10 Softmax - 2 0 #11 Output - 0 0 Summary. Total ops (MAC): 6610752 Prediction time :109309us Efficiency 60.47 ops/us Total Memory cost (Network and NNoM): 32876","title":"model_stat"},{"location":"api_evaluation/#nnom_predic_one","text":"int32_t nnom_predic_one(nnom_model_t *m, int8_t *input, int8_t *output); To predict one set of input data. Arguments m: the model to run prediction (evaluation). input: the data to predict. Can pass the buffer which has passed to Input Layer. output: the data to predict. Can pass the buffer which has passed to Output Layer. Return The predicted label in digit.","title":"nnom_predic_one"},{"location":"api_evaluation/#prediction_create","text":"nnom_predic_t *prediction_create(nnom_model_t *m, int8_t *buf_prediction, size_t label_num, size_t top_k_size); This method create a prediction instance, which record mutiple parameters in the evaluation process. Arguments m: the model to run prediction (evaluation). buf_prediction: the output buffer of the model, which should be the output of Softmax. Size equal to the size of class. label_num: the number of labels (the number of classifications). top_k_size: the Top-k that wants to evaluate. Return - The prediction instance. Note Check later examples.","title":"prediction_create"},{"location":"api_evaluation/#prediction_run","text":"int32_t prediction_run(nnom_predic_t *pre, uint32_t label); To run a prodiction with the new data (feed by user to the input_buffer which passed to Input layer). Arguments pre: the prediction instance created by prediction_create() . label: the true label of this data. Return - The top-1 prediction of current data.","title":"prediction_run"},{"location":"api_evaluation/#prediction_end","text":"void prediction_end(nnom_predic_t *pre); To mark the prediction has done. Arguments pre: the prediction instance created by prediction_create() .","title":"prediction_end"},{"location":"api_evaluation/#prediction_delete","text":"void predicetion_delete(nnom_predic_t *pre); To free all resources. Arguments pre: the prediction instance created by prediction_create() .","title":"prediction_delete"},{"location":"api_evaluation/#prediction_matrix","text":"void prediction_matrix(nnom_predic_t *pre); To print a confusion matrix when the prediction is done. Arguments pre: the prediction instance created by prediction_create() . Example Confusion matrix: predic 0 1 2 3 4 5 6 7 8 9 10 actual 0 | 395 1 0 0 2 0 0 0 0 0 21 | 94% 1 | 0 355 4 7 1 0 0 0 0 3 35 | 87% 2 | 0 3 325 2 1 0 7 29 3 2 53 | 76% 3 | 0 33 1 335 1 0 0 0 0 2 34 | 82% 4 | 6 0 1 0 371 3 0 0 0 0 31 | 90% 5 | 0 0 2 0 6 347 0 0 0 0 41 | 87% 6 | 0 1 5 8 0 0 322 4 0 0 56 | 81% 7 | 0 3 23 0 3 0 9 330 1 1 32 | 82% 8 | 0 0 5 2 0 0 0 0 343 4 57 | 83% 9 | 0 40 4 10 1 0 0 0 0 304 43 | 75% 10 | 4 61 16 34 28 17 14 6 12 37 6702 | 96%","title":"prediction_matrix"},{"location":"api_evaluation/#prediction_top_k","text":"void prediction_top_k(nnom_predic_t *pre); To print a Top-k when the prediction is done. Arguments pre: the prediction instance created by prediction_create() . Example Top 1 Accuracy: 92.03% Top 2 Accuracy: 96.39% Top 3 Accuracy: 97.38% Top 4 Accuracy: 97.85% Top 5 Accuracy: 98.13% Top 6 Accuracy: 98.40% Top 7 Accuracy: 98.59% Top 8 Accuracy: 98.88% Top 9 Accuracy: 99.14% Top 10 Accuracy: 99.60%","title":"prediction_top_k"},{"location":"api_evaluation/#prediction_summary","text":"void prediction_summary(nnom_predic_t *pre); To print a summary when the prediction is done. Arguments pre: the prediction instance created by prediction_create() . Example Prediction summary: Test frames: 11005 Test running time: 1598 sec Model running time: 1364908 ms Average prediction time: 124026 us Average effeciency: 53.30 ops/us Average frame rate: 8.0 Hz","title":"prediction_summary"},{"location":"api_evaluation/#example","text":"How to evaluate After a model has been compiled, then it is ready to be evaluated. The evaluation gose through a few steps Create a instance using prediction_create() Feed data one by one to the input buffer, then call prediction_run(pre) with true label. When all data has predicted, call prediction_end() . Then you can use prediction_matrix() , prediction_top_k() , and prediction_summary() to see the results. In addition, you can call model_stat() to see the performance of the last prediction. After all, call prediction_delete() to release all memory. How to implement in real-life Please check the UCI_HAR example for coding detail with RT-Thread and Y-modem. msh > \\ | / - RT - Thread Operating System / | \\ 4.0.0 build Mar 28 2019 2006 - 2018 Copyright by rt-thread team RTT Control Block Detection Address is 0x20000b3c msh > INFO: Start compile... Layer Activation output shape ops memory mem life-time ---------------------------------------------------------------------------------------------- Input - - ( 62, 12, 1) 0 ( 744, 744, 0) 1 - - - - - - - Conv2D - ReLU - ( 60, 10, 32) 172800 ( 744,19200, 1152) 1 1 - - - - - - MaxPool - - ( 30, 9, 32) 0 (19200, 8640, 0) 1 - 1 - - - - - Conv2D - ReLU - ( 28, 7, 64) 3612672 ( 8640,12544, 2304) 1 1 - - - - - - MaxPool - - ( 14, 6, 64) 0 (12544, 5376, 0) 1 - 1 - - - - - Conv2D - ReLU - ( 12, 4, 96) 2654208 ( 5376, 4608, 3456) 1 1 - - - - - - MaxPool - - ( 6, 3, 96) 0 ( 4608, 1728, 0) 1 - 1 - - - - - Conv2D - - ( 6, 3, 11) 171072 ( 1728, 198, 396) 1 1 - - - - - - GL_SumPool - - ( 1, 1, 11) 0 ( 198, 11, 44) 1 - 1 - - - - - Softmax - - ( 1, 1, 11) 0 ( 11, 11, 0) - 1 - - - - - - Output - - ( 11, 1, 1) 0 ( 11, 11, 0) 1 - - - - - - - ---------------------------------------------------------------------------------------------- INFO: memory analysis result Block0: 3456 Block1: 8640 Block2: 19200 Block3: 0 Block4: 0 Block5: 0 Block6: 0 Block7: 0 Total memory cost by network buffers: 31296 bytes msh >nn nn_stat msh >nn_stat Print running stat.. Layer(#) - Time(us) ops(MACs) ops/us -------------------------------------------------------- #1 Input - 9 0 #2 Conv2D - 8294 172800 20.83 #3 MaxPool - 5750 0 #4 Conv2D - 50089 3612672 72.12 #5 MaxPool - 3619 0 #6 Conv2D - 35890 2654208 73.95 #7 MaxPool - 1204 0 #8 Conv2D - 4411 171072 38.78 #9 GL_SumPool - 30 0 #10 Softmax - 3 0 #11 Output - 0 0 Summary. Total ops (MAC): 6610752 Prediction time :109299us Efficiency 60.48 ops/us Total Memory cost (Network and NNoM): 32876 msh >pre predic msh >predic Please select the NNoM binary test file and use Ymodem-128/1024 to send. CCCC Prediction done. Prediction summary: Test frames: 11005 Test running time: 1598 sec Model running time: 1364908 ms Average prediction time: 124026 us Average effeciency: 53.30 ops/us Average frame rate: 8.0 Hz Top 1 Accuracy: 92.03% Top 2 Accuracy: 96.39% Top 3 Accuracy: 97.38% Top 4 Accuracy: 97.85% Top 5 Accuracy: 98.13% Top 6 Accuracy: 98.40% Top 7 Accuracy: 98.59% Top 8 Accuracy: 98.88% Top 9 Accuracy: 99.14% Top 10 Accuracy: 99.60% Confusion matrix: predic 0 1 2 3 4 5 6 7 8 9 10 actual 0 | 395 1 0 0 2 0 0 0 0 0 21 | 94% 1 | 0 355 4 7 1 0 0 0 0 3 35 | 87% 2 | 0 3 325 2 1 0 7 29 3 2 53 | 76% 3 | 0 33 1 335 1 0 0 0 0 2 34 | 82% 4 | 6 0 1 0 371 3 0 0 0 0 31 | 90% 5 | 0 0 2 0 6 347 0 0 0 0 41 | 87% 6 | 0 1 5 8 0 0 322 4 0 0 56 | 81% 7 | 0 3 23 0 3 0 9 330 1 1 32 | 82% 8 | 0 0 5 2 0 0 0 0 343 4 57 | 83% 9 | 0 40 4 10 1 0 0 0 0 304 43 | 75% 10 | 4 61 16 34 28 17 14 6 12 37 6702 | 96% msh > OO: command not found. msh > msh >nn nn_stat msh >nn_stat Print running stat.. Layer(#) - Time(us) ops(MACs) ops/us -------------------------------------------------------- #1 Input - 9 0 #2 Conv2D - 8292 172800 20.83 #3 MaxPool - 5753 0 #4 Conv2D - 50095 3612672 72.11 #5 MaxPool - 3617 0 #6 Conv2D - 35893 2654208 73.94 #7 MaxPool - 1206 0 #8 Conv2D - 4412 171072 38.77 #9 GL_SumPool - 30 0 #10 Softmax - 2 0 #11 Output - 0 0 Summary. Total ops (MAC): 6610752 Prediction time :109309us Efficiency 60.47 ops/us Total Memory cost (Network and NNoM): 32876 msh >","title":"Example"},{"location":"api_layers/","text":"Layer APIs Layers APIs are listed in nnom_layers.h 1D/2D operations are both working with (H, W, C) format, known as \"channel last\". When working with 1D operations, the H for all the shapes must be 1 constantly. Input nnom_layer_t* Input(nnom_shape_t input_shape, * p_buf); A model must start with a Input layer for copying inputd data from user space to NNoM space. Arguments input_shape: the shape of input. p_buf: the data buf in user space. Return The layer instance Output nnom_layer_t* Output(nnom_shape_t output_shape* p_buf); Output layer is to copy the result from NNoM space to user space. Arguments output_shape: the shape of output. p_buf: the data buf in user space. Return The layer instance Conv2D nnom_layer_t* Conv2D(uint32_t filters, nnom_shape_t k, nnom_shape_t s, nnom_padding_t pad, nnom_weight_t *w, nnom_bias_t *b); This funtion is for 1D or 2D, mutiple channels convolution. Arguments filters: the number of filters. or the channels of the output spaces. k (kernel): the kernel shape, which is returned by kernel() s (stride): the stride shape, which is returned by stride() pad (padding): the padding method PADDING_SAME or PADDING_VALID w (weights) / b (bias) : weights and bias constants and shits. Generated in weights.h Return The layer instance Notes When it is used for 1D convolution, the H should be set to 1 constantly in kernel and stride. DW_Conv2D nnom_layer_t* DW_Conv2D(uint32_t multiplier, nnom_shape_t k, nnom_shape_t s, nnom_padding_t pad, nnom_weight_t *w, nnom_bias_t *b); This funtion is for 1D or 2D, mutiple channels depthwise convolution. Arguments mutiplier: the number of mutiplier. Currently only support mutiplier=1 k (kernel): the kernel shape, which is returned by kernel() s (stride): the stride shape, which is returned by stride() pad (padding): the padding method PADDING_SAME or PADDING_VALID w (weights) / b (bias) : weights and bias constants and shits. Generated in weights.h Return The layer instance Notes When it is used for 1D convolution, the H should be set to 1 constantly in kernel and stride. Dense nnom_layer_t* Dense(size_t output_unit, nnom_weight_t *w, nnom_bias_t *b); Arguments output_unit: the number of output unit. w (weights) / b (bias) : weights and bias constants and shits. Generated in weights.h Return The layer instance Lambda // Lambda Layers // layer.run() , required // layer.oshape(), optional, call default_output_shape() if left NULL // layer.free() , optional, called while model is deleting, to free private resources // parameters , private parameters for run method, left NULL if not needed. nnom_layer_t *Lambda(nnom_status_t (*run)(nnom_layer_t *), nnom_status_t (*oshape)(nnom_layer_t *), nnom_status_t (*free)(nnom_layer_t *), void *parameters); Lambda layer is an anonymous layer (interface), which allows user to do customized operation between the layer's input data and output data. Arguments - ( run)(nnom_layer_t ): or so called run method, is the method to do the customized operation. - ( oshape)(nnom_layer_t ): is to calculate the output shape according to the input shape during compiling. If this method is not presented, the input shape will be passed to the output shape. - ( free)(nnom_layer_t ): is to free the resources allocated by users. this will be called when deleting models. Leave it NULL if no resources need to be released. - parameters: is the pointer to user configurations. User can access to it in all three methods above. Return The layer instance Notes When oshape() is present, please refer to example of other similar layers. This method is called in compiling, thus it can also do works other than calculating output shape only. An exmaple is the global_pooling_output_shape() fill in the parameters left by 'GlobalXXXPool()' Examples Conv2D: //For 1D convolution nnom_layer_t *layer; layer = Conv2D(32, kernel(1, 5), stride(1, 2), PADDING_VALID, &conv2d_3_w, &conv2d_3_b);` DW_Conv2D: nnom_layer_t *layer; layer = Conv2D(32, kernel(3, 3), stride(1, 1), PADDING_VALID, &conv2d_3_w, &conv2d_3_b);` Dense: nnom_layer_t *layer; layer = Dense(32, &dense_w, &dense_b); Lambda: This example shows how to use Lambda layer to copy data from the input buffer to the output buffer. nnom_status_t lambda_run(layer) { memcpy(layer->output, layer->input, sizeof(inputshape); return NN_SUCCESS; } main() { layer *x, *input; x = model.hook(Lambda(lambda_run, NULL, NULL, NULL), input); }","title":"Core Layers"},{"location":"api_layers/#layer-apis","text":"Layers APIs are listed in nnom_layers.h 1D/2D operations are both working with (H, W, C) format, known as \"channel last\". When working with 1D operations, the H for all the shapes must be 1 constantly.","title":"Layer APIs"},{"location":"api_layers/#input","text":"nnom_layer_t* Input(nnom_shape_t input_shape, * p_buf); A model must start with a Input layer for copying inputd data from user space to NNoM space. Arguments input_shape: the shape of input. p_buf: the data buf in user space. Return The layer instance","title":"Input"},{"location":"api_layers/#output","text":"nnom_layer_t* Output(nnom_shape_t output_shape* p_buf); Output layer is to copy the result from NNoM space to user space. Arguments output_shape: the shape of output. p_buf: the data buf in user space. Return The layer instance","title":"Output"},{"location":"api_layers/#conv2d","text":"nnom_layer_t* Conv2D(uint32_t filters, nnom_shape_t k, nnom_shape_t s, nnom_padding_t pad, nnom_weight_t *w, nnom_bias_t *b); This funtion is for 1D or 2D, mutiple channels convolution. Arguments filters: the number of filters. or the channels of the output spaces. k (kernel): the kernel shape, which is returned by kernel() s (stride): the stride shape, which is returned by stride() pad (padding): the padding method PADDING_SAME or PADDING_VALID w (weights) / b (bias) : weights and bias constants and shits. Generated in weights.h Return The layer instance Notes When it is used for 1D convolution, the H should be set to 1 constantly in kernel and stride.","title":"Conv2D"},{"location":"api_layers/#dw_conv2d","text":"nnom_layer_t* DW_Conv2D(uint32_t multiplier, nnom_shape_t k, nnom_shape_t s, nnom_padding_t pad, nnom_weight_t *w, nnom_bias_t *b); This funtion is for 1D or 2D, mutiple channels depthwise convolution. Arguments mutiplier: the number of mutiplier. Currently only support mutiplier=1 k (kernel): the kernel shape, which is returned by kernel() s (stride): the stride shape, which is returned by stride() pad (padding): the padding method PADDING_SAME or PADDING_VALID w (weights) / b (bias) : weights and bias constants and shits. Generated in weights.h Return The layer instance Notes When it is used for 1D convolution, the H should be set to 1 constantly in kernel and stride.","title":"DW_Conv2D"},{"location":"api_layers/#dense","text":"nnom_layer_t* Dense(size_t output_unit, nnom_weight_t *w, nnom_bias_t *b); Arguments output_unit: the number of output unit. w (weights) / b (bias) : weights and bias constants and shits. Generated in weights.h Return The layer instance","title":"Dense"},{"location":"api_layers/#lambda","text":"// Lambda Layers // layer.run() , required // layer.oshape(), optional, call default_output_shape() if left NULL // layer.free() , optional, called while model is deleting, to free private resources // parameters , private parameters for run method, left NULL if not needed. nnom_layer_t *Lambda(nnom_status_t (*run)(nnom_layer_t *), nnom_status_t (*oshape)(nnom_layer_t *), nnom_status_t (*free)(nnom_layer_t *), void *parameters); Lambda layer is an anonymous layer (interface), which allows user to do customized operation between the layer's input data and output data. Arguments - ( run)(nnom_layer_t ): or so called run method, is the method to do the customized operation. - ( oshape)(nnom_layer_t ): is to calculate the output shape according to the input shape during compiling. If this method is not presented, the input shape will be passed to the output shape. - ( free)(nnom_layer_t ): is to free the resources allocated by users. this will be called when deleting models. Leave it NULL if no resources need to be released. - parameters: is the pointer to user configurations. User can access to it in all three methods above. Return The layer instance Notes When oshape() is present, please refer to example of other similar layers. This method is called in compiling, thus it can also do works other than calculating output shape only. An exmaple is the global_pooling_output_shape() fill in the parameters left by 'GlobalXXXPool()'","title":"Lambda"},{"location":"api_layers/#examples","text":"Conv2D: //For 1D convolution nnom_layer_t *layer; layer = Conv2D(32, kernel(1, 5), stride(1, 2), PADDING_VALID, &conv2d_3_w, &conv2d_3_b);` DW_Conv2D: nnom_layer_t *layer; layer = Conv2D(32, kernel(3, 3), stride(1, 1), PADDING_VALID, &conv2d_3_w, &conv2d_3_b);` Dense: nnom_layer_t *layer; layer = Dense(32, &dense_w, &dense_b); Lambda: This example shows how to use Lambda layer to copy data from the input buffer to the output buffer. nnom_status_t lambda_run(layer) { memcpy(layer->output, layer->input, sizeof(inputshape); return NN_SUCCESS; } main() { layer *x, *input; x = model.hook(Lambda(lambda_run, NULL, NULL, NULL), input); }","title":"Examples"},{"location":"api_merge/","text":"Merging Methods Merge methods (layers) are use to merge 2 or more layer's output using the methods list below. These methods are also layers which return a layer instance. However, they normally take two or more layer's output and \"merge\" them into one output. These layer instance must be passed to either model.merge(method, in1, in2) or model.mergex(method, num_of_input, in1, in2, 1n3 ...) . An example will be to concat the Inception structure. Concat nnom_layer_t* Concat(int8_t axis); Concatenate mutiple input on the selected axis. Arguments axis: the axis number to concatenate in HWC format. The axis could be nagative, such as '-1' indicate the last one axis which is 'Channel'. Return The concat layer instance Notes The concatenated axis can be different in those input layers. Other axes must be same. Mult nnom_layer_t* Mult(void); Element wise mutiplication in all the inputs Return The mult layer instance Add nnom_layer_t* Add(void); Element wise addition in all the inputs. Return The add layer instance Sub nnom_layer_t* Sub(void); Element wise substraction in all the inputs. Return The sub layer instance Example Channelwise concat for Inception input_layer = Input(shape(INPUT_HIGHT, INPUT_WIDTH, INPUT_CH), nnom_input_data); // conv2d - 1 - inception x1 = model.hook(Conv2D(16, kernel(1, 5), stride(1, 1), PADDING_SAME, &c2_w, &c2_b), x); x1 = model.hook(MaxPool(kernel(1, 2), stride(1, 2), PADDING_VALID), x1); // conv2d - 2 - inception x2 = model.hook(Conv2D(16, kernel(1, 3), stride(1, 1), PADDING_SAME, &c3_w, &c3_b), x); x2 = model.hook(MaxPool(kernel(1, 2), stride(1, 2), PADDING_VALID), x2); // maxpool - 3 - inception x3 = model.hook(MaxPool(kernel(1, 2), stride(1, 2), PADDING_VALID), x); // concatenate x = model.mergex(Concat(-1), 3, x1, x2, x3); // flatten x = model.hook(Flatten(), x); ...","title":"Merges"},{"location":"api_merge/#merging-methods","text":"Merge methods (layers) are use to merge 2 or more layer's output using the methods list below. These methods are also layers which return a layer instance. However, they normally take two or more layer's output and \"merge\" them into one output. These layer instance must be passed to either model.merge(method, in1, in2) or model.mergex(method, num_of_input, in1, in2, 1n3 ...) . An example will be to concat the Inception structure.","title":"Merging Methods"},{"location":"api_merge/#concat","text":"nnom_layer_t* Concat(int8_t axis); Concatenate mutiple input on the selected axis. Arguments axis: the axis number to concatenate in HWC format. The axis could be nagative, such as '-1' indicate the last one axis which is 'Channel'. Return The concat layer instance Notes The concatenated axis can be different in those input layers. Other axes must be same.","title":"Concat"},{"location":"api_merge/#mult","text":"nnom_layer_t* Mult(void); Element wise mutiplication in all the inputs Return The mult layer instance","title":"Mult"},{"location":"api_merge/#add","text":"nnom_layer_t* Add(void); Element wise addition in all the inputs. Return The add layer instance","title":"Add"},{"location":"api_merge/#sub","text":"nnom_layer_t* Sub(void); Element wise substraction in all the inputs. Return The sub layer instance","title":"Sub"},{"location":"api_merge/#example","text":"Channelwise concat for Inception input_layer = Input(shape(INPUT_HIGHT, INPUT_WIDTH, INPUT_CH), nnom_input_data); // conv2d - 1 - inception x1 = model.hook(Conv2D(16, kernel(1, 5), stride(1, 1), PADDING_SAME, &c2_w, &c2_b), x); x1 = model.hook(MaxPool(kernel(1, 2), stride(1, 2), PADDING_VALID), x1); // conv2d - 2 - inception x2 = model.hook(Conv2D(16, kernel(1, 3), stride(1, 1), PADDING_SAME, &c3_w, &c3_b), x); x2 = model.hook(MaxPool(kernel(1, 2), stride(1, 2), PADDING_VALID), x2); // maxpool - 3 - inception x3 = model.hook(MaxPool(kernel(1, 2), stride(1, 2), PADDING_VALID), x); // concatenate x = model.mergex(Concat(-1), 3, x1, x2, x3); // flatten x = model.hook(Flatten(), x); ...","title":"Example"},{"location":"api_model/","text":"Model APIs NNoM support Sequential model and Functional model whitch are similar to Keras. Model is a minimum runable object in NNoM. Here list the Model APIs that used for create, compile and run a model. new_model nnom_model_t *new_model(nnom_model_t *m); This method is to create or initiate a model instance. Arguments m: the model instance that need to be initiated. If NULL is passed to it, the method will create a new model instance. Return The created model instance or the model_delete void model_delete(nnom_model_t *m); Delete and free all the resources created with the model. Arguments m: the model instance. sequencial_compile nnom_status_t sequencial_compile(nnom_model_t *m); Compile a sequencial model which is constructed by sequencial construction APIs. Arguments m: the model instance for compile. Return Status of compiling. model_compile nnom_status_t model_compile(nnom_model_t *m, nnom_layer_t* input, nnom_layer_t* output); Arguments m: the model instance for compile. input: the specified input layer instance. output: the specified output layer instance. If left NULL , the all layers will be compile. Return Status of compiling. model_run nnom_status_t model_run(nnom_model_t *m); To run all the layers inside the model. Arguments m: the model instance for compile. input: the specified input layer instance. output: the specified output layer instance. If left NULL , the all layers will be compile. Return The result of layer running. Note User must fill in the input buffer which has passed to the input layer before run the model. The input layer then copy the data from user space to NNoM memory space to run the model. model_run_to nnom_status_t model_run_to(nnom_model_t *m, nnom_layer_t *end_layer); Run partly to the specified layer. Arguments m: the model instance for compile. end_layer: the layer instance to stop. Return The result of layer running. Examples Sequential model /* nnom model */ int8_t input_data[784]; int8_t output_data[10]; void sequencial_model(void) { nnom_model_t model; new_model(&model); model.add(&model, Input(shape(784, 1, 1), input_data)); model.add(&model, Flatten()); model.add(&model, Dense(100, &w1, &b1)); model.add(&model, Dense(10, &w2, &b2)); model.add(&model, Softmax()) model.add(&model, Output(shape(10, 1, 1), output_data)); sequencial_compile(&model); while(1) { feed_data(&input_data); model_run(&model); // evaluate on output_data[] ... } } Functional model /* nnom model */ int8_t input_data[784]; int8_t output_data[10]; void functional_model(void) { static nnom_model_t model; nnom_layer_t *input, *x; new_model(&model); input = Input(shape(784, 1, 1), input_data); x = model.hook(Flatten(), input); x = model.hook(Dense(100, &w1, &b1), x) x = model.hook(Dense(10, &w2, &b2), x) x = model.hook(Softmax(), x) x = model.hook(Output(shape(10, 1, 1), output_data), x); // compile these layers into the model. model_compile(&model, input, x); while(1) { feed_data(&input_data); model_run(&model); // evaluate on output_data[] ... } }","title":"Model"},{"location":"api_model/#model-apis","text":"NNoM support Sequential model and Functional model whitch are similar to Keras. Model is a minimum runable object in NNoM. Here list the Model APIs that used for create, compile and run a model.","title":"Model APIs"},{"location":"api_model/#new_model","text":"nnom_model_t *new_model(nnom_model_t *m); This method is to create or initiate a model instance. Arguments m: the model instance that need to be initiated. If NULL is passed to it, the method will create a new model instance. Return The created model instance or the","title":"new_model"},{"location":"api_model/#model_delete","text":"void model_delete(nnom_model_t *m); Delete and free all the resources created with the model. Arguments m: the model instance.","title":"model_delete"},{"location":"api_model/#sequencial_compile","text":"nnom_status_t sequencial_compile(nnom_model_t *m); Compile a sequencial model which is constructed by sequencial construction APIs. Arguments m: the model instance for compile. Return Status of compiling.","title":"sequencial_compile"},{"location":"api_model/#model_compile","text":"nnom_status_t model_compile(nnom_model_t *m, nnom_layer_t* input, nnom_layer_t* output); Arguments m: the model instance for compile. input: the specified input layer instance. output: the specified output layer instance. If left NULL , the all layers will be compile. Return Status of compiling.","title":"model_compile"},{"location":"api_model/#model_run","text":"nnom_status_t model_run(nnom_model_t *m); To run all the layers inside the model. Arguments m: the model instance for compile. input: the specified input layer instance. output: the specified output layer instance. If left NULL , the all layers will be compile. Return The result of layer running. Note User must fill in the input buffer which has passed to the input layer before run the model. The input layer then copy the data from user space to NNoM memory space to run the model.","title":"model_run"},{"location":"api_model/#model_run_to","text":"nnom_status_t model_run_to(nnom_model_t *m, nnom_layer_t *end_layer); Run partly to the specified layer. Arguments m: the model instance for compile. end_layer: the layer instance to stop. Return The result of layer running.","title":"model_run_to"},{"location":"api_model/#examples","text":"Sequential model /* nnom model */ int8_t input_data[784]; int8_t output_data[10]; void sequencial_model(void) { nnom_model_t model; new_model(&model); model.add(&model, Input(shape(784, 1, 1), input_data)); model.add(&model, Flatten()); model.add(&model, Dense(100, &w1, &b1)); model.add(&model, Dense(10, &w2, &b2)); model.add(&model, Softmax()) model.add(&model, Output(shape(10, 1, 1), output_data)); sequencial_compile(&model); while(1) { feed_data(&input_data); model_run(&model); // evaluate on output_data[] ... } } Functional model /* nnom model */ int8_t input_data[784]; int8_t output_data[10]; void functional_model(void) { static nnom_model_t model; nnom_layer_t *input, *x; new_model(&model); input = Input(shape(784, 1, 1), input_data); x = model.hook(Flatten(), input); x = model.hook(Dense(100, &w1, &b1), x) x = model.hook(Dense(10, &w2, &b2), x) x = model.hook(Softmax(), x) x = model.hook(Output(shape(10, 1, 1), output_data), x); // compile these layers into the model. model_compile(&model, input, x); while(1) { feed_data(&input_data); model_run(&model); // evaluate on output_data[] ... } }","title":"Examples"},{"location":"api_pooling/","text":"Pooling Layers Pooling Layers are listed in nnom_layers.h 1D/2D operations are both working with (H, W, C) format, known as \"channel last\". When working with 1D operations, the H for all the shapes must be 1 constantly. MaxPool nnom_layer_t* MaxPool(nnom_shape_t k, nnom_shape_t s, nnom_padding_t pad); This funtion is for 1D or 2D, mutiple channels max pooling. Arguments k (kernel): the kernel shape, which is returned by kernel() s (stride): the stride shape, which is returned by stride() pad (padding): the padding method PADDING_SAME or PADDING_VALID AvgPool nnom_layer_t* AvgPool(nnom_shape_t k, nnom_shape_t s, nnom_padding_t pad); This funtion is for 1D or 2D, mutiple channels average pooling. Arguments k (kernel): the kernel shape, which is returned by kernel() s (stride): the stride shape, which is returned by stride() pad (padding): the padding method PADDING_SAME or PADDING_VALID Notes Average pooling is not recommended to us with fixed-point model (such as here). Small values will be lost when the sum is devided. CMSIS-NN currently does not support changing the output shifting in average pooling from input to output. However, if the average pooling is the second last layer right before softmax layer, you can still use average pooling for training and then use sumpooling in MCU. the only different is sumpooling will not divide the sum directly but looking for a best shift (dynamic shifting) to cover the largest number. SumPool nnom_layer_t* SumPool(nnom_shape_t k, nnom_shape_t s, nnom_padding_t pad); This funtion is for 1D or 2D, mutiple channels Sum pooling. This is a better alternative to average pooling WHEN deploy trained model into NNoM. The output shift for sumpool in NNoM is dynamic, means that this pooling can only place before softmax layer. Arguments k (kernel): the kernel shape, which is returned by kernel() s (stride): the stride shape, which is returned by stride() pad (padding): the padding method PADDING_SAME or PADDING_VALID GlobalMaxPool nnom_layer_t *GlobalMaxPool(void); Global Max Pooling GlobalAvgPool nnom_layer_t *GlobalAvgPool(void); Global Average Pooling. Due to the same reason as discussed in Average pooling, it is recommended to replace this layer by GlobalSumPool() when the layer it she second last layer, and before the softmax layer. If you used generate_model() to convert your keras model to NNoM, this layer will be automaticly replaced by GlobalSumPool() when above conditions has met. GlobalSumPool nnom_layer_t *GlobalSumPool(void);","title":"RTT"},{"location":"api_pooling/#pooling-layers","text":"Pooling Layers are listed in nnom_layers.h 1D/2D operations are both working with (H, W, C) format, known as \"channel last\". When working with 1D operations, the H for all the shapes must be 1 constantly.","title":"Pooling Layers"},{"location":"api_pooling/#maxpool","text":"nnom_layer_t* MaxPool(nnom_shape_t k, nnom_shape_t s, nnom_padding_t pad); This funtion is for 1D or 2D, mutiple channels max pooling. Arguments k (kernel): the kernel shape, which is returned by kernel() s (stride): the stride shape, which is returned by stride() pad (padding): the padding method PADDING_SAME or PADDING_VALID","title":"MaxPool"},{"location":"api_pooling/#avgpool","text":"nnom_layer_t* AvgPool(nnom_shape_t k, nnom_shape_t s, nnom_padding_t pad); This funtion is for 1D or 2D, mutiple channels average pooling. Arguments k (kernel): the kernel shape, which is returned by kernel() s (stride): the stride shape, which is returned by stride() pad (padding): the padding method PADDING_SAME or PADDING_VALID Notes Average pooling is not recommended to us with fixed-point model (such as here). Small values will be lost when the sum is devided. CMSIS-NN currently does not support changing the output shifting in average pooling from input to output. However, if the average pooling is the second last layer right before softmax layer, you can still use average pooling for training and then use sumpooling in MCU. the only different is sumpooling will not divide the sum directly but looking for a best shift (dynamic shifting) to cover the largest number.","title":"AvgPool"},{"location":"api_pooling/#sumpool","text":"nnom_layer_t* SumPool(nnom_shape_t k, nnom_shape_t s, nnom_padding_t pad); This funtion is for 1D or 2D, mutiple channels Sum pooling. This is a better alternative to average pooling WHEN deploy trained model into NNoM. The output shift for sumpool in NNoM is dynamic, means that this pooling can only place before softmax layer. Arguments k (kernel): the kernel shape, which is returned by kernel() s (stride): the stride shape, which is returned by stride() pad (padding): the padding method PADDING_SAME or PADDING_VALID","title":"SumPool"},{"location":"api_pooling/#globalmaxpool","text":"nnom_layer_t *GlobalMaxPool(void); Global Max Pooling","title":"GlobalMaxPool"},{"location":"api_pooling/#globalavgpool","text":"nnom_layer_t *GlobalAvgPool(void); Global Average Pooling. Due to the same reason as discussed in Average pooling, it is recommended to replace this layer by GlobalSumPool() when the layer it she second last layer, and before the softmax layer. If you used generate_model() to convert your keras model to NNoM, this layer will be automaticly replaced by GlobalSumPool() when above conditions has met.","title":"GlobalAvgPool"},{"location":"api_pooling/#globalsumpool","text":"nnom_layer_t *GlobalSumPool(void);","title":"GlobalSumPool"},{"location":"api_properties/","text":"Properties Properties include some basic properties such as shape of the data buffer, Q-format of the data. typesdef typedef struct _nnom_shape { nnom_shape_data_t h, w, c; } nnom_shape_t; typedef struct _nnom_weights { const void *p_value; size_t shift; // the right shift for output } nnom_weight_t; typedef struct _nnom_bias { const void *p_value; size_t shift; // the left shift for bias } nnom_bias_t; typedef struct _nnom_qformat { int8_t n, m; } nnom_qformat_t; shape nnom_shape_t shape(size_t h, size_t w, size_t c); Arguments h: size of H, or number of row, or y axis in image. w: size of W, or number of row, or x axis in image. c: size of channel. Return A shape instance. kernel nnom_shape_t kernel(size_t h, size_t w); Use in pooling or convolutional layer to specified the kernel size. Arguments h: size of kernel in H, or number of row, or y axis in image. w: size of kernel in W, or number of row, or x axis in image. Return A shape instance. stride nnom_shape_t stride(size_t h, size_t w); Use in pooling or convolutional layer to specified the stride size. Arguments h: size of stride in H, or number of row, or y axis in image. w: size of stride in W, or number of row, or x axis in image. Return A shape instance. qformat nnom_qformat_t qformat(int8_t m, int8_t n); Arguments m: the integer bitwidth. n: the fractional bitwidth. Return A nnom_qformat_t inistance. Notes The Q-format within model is currently handled by Python script nnom_utils.py . This function will be deprecated. shape_size size_t shape_size(nnom_shape_t *s); Calculate the size from a shape. size = s.h * s.w * s.c; Arguments s: the shape to calculate. Return The total size of the shape.","title":"Properties"},{"location":"api_properties/#properties","text":"Properties include some basic properties such as shape of the data buffer, Q-format of the data.","title":"Properties"},{"location":"api_properties/#typesdef","text":"typedef struct _nnom_shape { nnom_shape_data_t h, w, c; } nnom_shape_t; typedef struct _nnom_weights { const void *p_value; size_t shift; // the right shift for output } nnom_weight_t; typedef struct _nnom_bias { const void *p_value; size_t shift; // the left shift for bias } nnom_bias_t; typedef struct _nnom_qformat { int8_t n, m; } nnom_qformat_t;","title":"typesdef"},{"location":"api_properties/#shape","text":"nnom_shape_t shape(size_t h, size_t w, size_t c); Arguments h: size of H, or number of row, or y axis in image. w: size of W, or number of row, or x axis in image. c: size of channel. Return A shape instance.","title":"shape"},{"location":"api_properties/#kernel","text":"nnom_shape_t kernel(size_t h, size_t w); Use in pooling or convolutional layer to specified the kernel size. Arguments h: size of kernel in H, or number of row, or y axis in image. w: size of kernel in W, or number of row, or x axis in image. Return A shape instance.","title":"kernel"},{"location":"api_properties/#stride","text":"nnom_shape_t stride(size_t h, size_t w); Use in pooling or convolutional layer to specified the stride size. Arguments h: size of stride in H, or number of row, or y axis in image. w: size of stride in W, or number of row, or x axis in image. Return A shape instance.","title":"stride"},{"location":"api_properties/#qformat","text":"nnom_qformat_t qformat(int8_t m, int8_t n); Arguments m: the integer bitwidth. n: the fractional bitwidth. Return A nnom_qformat_t inistance. Notes The Q-format within model is currently handled by Python script nnom_utils.py . This function will be deprecated.","title":"qformat"},{"location":"api_properties/#shape_size","text":"size_t shape_size(nnom_shape_t *s); Calculate the size from a shape. size = s.h * s.w * s.c; Arguments s: the shape to calculate. Return The total size of the shape.","title":"shape_size"}]}